{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu==2.9 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (3.12.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (1.12.1)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (2.9.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (1.27.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (1.6.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (47.3.1.post20200622)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (2.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (1.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (1.1.2)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (0.27.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (14.0.6)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (1.12)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (21.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (3.1.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (2.9.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (0.2.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (1.21.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorflow-gpu==2.9) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (3.1.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (1.6.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (0.34.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (1.14.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (2.24.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (2.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (0.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (0.6.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from packaging->tensorflow-gpu==2.9) (3.0.9)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (4.1.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (2022.9.24)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (1.25.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (2.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from rsa<4.1,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow-gpu==2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python==4.5.1.48 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (4.5.1.48)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from opencv-python==4.5.1.48) (1.21.6)\n",
      "Requirement already satisfied: pandas==1.2.1 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from pandas==1.2.1) (1.21.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from pandas==1.2.1) (2022.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from pandas==1.2.1) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from python-dateutil>=2.7.3->pandas==1.2.1) (1.15.0)\n",
      "Requirement already satisfied: Pillow==8.1.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (8.1.0)\n",
      "Requirement already satisfied: pydicom==2.1.2 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: scikit-learn==0.24.1 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from scikit-learn==0.24.1) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from scikit-learn==0.24.1) (1.2.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from scikit-learn==0.24.1) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from scikit-learn==0.24.1) (1.21.6)\n",
      "Requirement already satisfied: scipy==1.6.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from scipy==1.6.0) (1.21.6)\n",
      "Requirement already satisfied: tqdm==4.56.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (4.56.0)\n",
      "Requirement already satisfied: tikzplotlib in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: webcolors in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tikzplotlib) (1.12)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tikzplotlib) (3.4.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tikzplotlib) (8.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from tikzplotlib) (1.21.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from matplotlib>=1.4.0->tikzplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib>=1.4.0->tikzplotlib) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from matplotlib>=1.4.0->tikzplotlib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib>=1.4.0->tikzplotlib) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->tikzplotlib) (1.15.0)\n",
      "Requirement already satisfied: efficientnet in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from efficientnet) (1.0.8)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from efficientnet) (0.18.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.21.6)\n",
      "Requirement already satisfied: scipy>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from scikit-image->efficientnet) (1.6.0)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from scikit-image->efficientnet) (8.1.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from scikit-image->efficientnet) (2021.4.8)\n",
      "Requirement already satisfied: imageio>=2.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from scikit-image->efficientnet) (2.9.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from scikit-image->efficientnet) (1.1.1)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from scikit-image->efficientnet) (3.4.2)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from scikit-image->efficientnet) (2.5.1)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.15.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (1.3.1)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from networkx>=2.0->scikit-image->efficientnet) (4.4.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: matplotlib>=3.1 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from seaborn) (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from seaborn) (1.21.6)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from seaborn) (1.2.1)\n",
      "Requirement already satisfied: typing_extensions; python_version < \"3.8\" in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from seaborn) (4.3.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from matplotlib>=3.1->seaborn) (8.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from matplotlib>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from matplotlib>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib>=3.1->seaborn) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib>=3.1->seaborn) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from pandas>=0.25->seaborn) (2022.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\keras-gpu\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.1->seaborn) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install opencv-python==4.5.1.48\n",
    "# !pip install pandas==1.2.1\n",
    "# !pip install Pillow==8.1.0\n",
    "# !pip install pydicom==2.1.2\n",
    "# !pip install scikit-learn==0.24.1\n",
    "# !pip install scipy==1.6.0\n",
    "# !pip install tqdm==4.56.0\n",
    "# !pip install tikzplotlib\n",
    "# !pip install efficientnet\n",
    "# !pip install seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pydicom\n",
    "import tikzplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import efficientnet.tfkeras as efn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.backend as K\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, Activation, Flatten, Input, BatchNormalization, GlobalAveragePooling2D, GaussianNoise, \n",
    "    Add, Conv2D, AveragePooling2D, LeakyReLU, Concatenate \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./Dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>Weeks</th>\n",
       "      <th>FVC</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SmokingStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>-4</td>\n",
       "      <td>2315</td>\n",
       "      <td>58.253649</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>5</td>\n",
       "      <td>2214</td>\n",
       "      <td>55.712129</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>7</td>\n",
       "      <td>2061</td>\n",
       "      <td>51.862104</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>9</td>\n",
       "      <td>2144</td>\n",
       "      <td>53.950679</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>11</td>\n",
       "      <td>2069</td>\n",
       "      <td>52.063412</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Patient  Weeks   FVC    Percent  Age   Sex SmokingStatus\n",
       "0  ID00007637202177411956430     -4  2315  58.253649   79  Male     Ex-smoker\n",
       "1  ID00007637202177411956430      5  2214  55.712129   79  Male     Ex-smoker\n",
       "2  ID00007637202177411956430      7  2061  51.862104   79  Male     Ex-smoker\n",
       "3  ID00007637202177411956430      9  2144  53.950679   79  Male     Ex-smoker\n",
       "4  ID00007637202177411956430     11  2069  52.063412   79  Male     Ex-smoker"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153.145377828922"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Percent'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.8775766716943"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Percent'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ex-smoker', 'Never smoked', 'Currently smokes'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.SmokingStatus.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tab(df):\n",
    "    \" return an array which contains each patient normalized age, sex and smoking status\"\n",
    "    vector = [(df.Age.values[0] - 30) / 30] \n",
    "    \n",
    "    if df.Sex.values[0].lower() == 'male':\n",
    "       vector.append(0)\n",
    "    else:\n",
    "       vector.append(1)\n",
    "    \n",
    "    if df.SmokingStatus.values[0] == 'Never smoked':\n",
    "        vector.extend([0,0])\n",
    "    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n",
    "        vector.extend([1,1])\n",
    "    elif df.SmokingStatus.values[0] == 'Currently smokes':\n",
    "        vector.extend([0,1])\n",
    "    else:\n",
    "        vector.extend([1,0])\n",
    "    return np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63755f9aa96c48f9ae888e8113e06032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\ipykernel_launcher.py:9: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if __name__ == \"__main__\":\n"
     ]
    }
   ],
   "source": [
    "A = {} \n",
    "TAB = {} \n",
    "P = [] \n",
    "for i, p in tqdm(enumerate(train.Patient.unique())): # i index, p patient id\n",
    "    sub = train.loc[train.Patient == p, :] # find all data (weeks, FVC, Percent, Age, Sex, SmokingStatus) of a unique patient\n",
    "    fvc = sub.FVC.values # fvc values of the patient during the follow-up\n",
    "    weeks = sub.Weeks.values # follow-up weeks\n",
    "    c = np.vstack([weeks, np.ones(len(weeks))]).T # create an array by the follow-up weeks of shape(len(weeks),2)\n",
    "    a, b = np.linalg.lstsq(c, fvc)[0] # least-square sol, a=gradient matrix, b=right hand matrix \n",
    "    \n",
    "    A[p] = a\n",
    "    TAB[p] = get_tab(sub)\n",
    "    P.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(path):\n",
    "    \" read DICOM dataset and return resize images of size (512,512,3)\"\n",
    "    d = pydicom.dcmread(path) # read and parse the CT scan images (in DICOM format)\n",
    "    resized_image = cv2.resize((d.pixel_array - d.RescaleIntercept) / (d.RescaleSlope * 1000), (512,512))\n",
    "    resized_image = np.stack((resized_image,)*3, axis = -1)\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class IGenerator(Sequence):\n",
    "    BAD_ID = ['ID00011637202177653955184', 'ID00052637202186188008618']\n",
    "    def __init__(self, keys, a, tab, batch_size):\n",
    "        \"key=patient, a=gradient matrix, tab=a particular patient's data\"\n",
    "        self.keys = [k for k in keys if k not in self.BAD_ID]\n",
    "        self.a = a\n",
    "        self.tab = tab\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.train_data = {}\n",
    "        for p in train.Patient.values:\n",
    "            self.train_data[p] = os.listdir(f'./Dataset//train/{p}/')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1000\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = []\n",
    "        a, tab = [], [] \n",
    "        keys = np.random.choice(self.keys, size = self.batch_size) # randomly chooses n=batch_size number of patients\n",
    "        for k in keys:\n",
    "            try:\n",
    "                i = np.random.choice(self.train_data[k], size=1)[0] # chooses some randomly images for kth patient\n",
    "                img = get_img(f'./Dataset//train/{k}/{i}') # resizes ith image of kth patient\n",
    "                x.append(img) # append kth patient's image data in the list x\n",
    "                a.append(self.a[k]) # append kth patient's gradinet in the list a\n",
    "                tab.append(self.tab[k]) # append kth patient's tabular data in the tab list\n",
    "            except:\n",
    "                print(k, i)\n",
    "       \n",
    "        x,a,tab = np.array(x), np.array(a), np.array(tab) # convert list to array\n",
    "        #x = np.expand_dims(x, axis=-1) \n",
    "        return [x, tab] , a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model, shape):\n",
    "    models_dict = {\n",
    "        'b0': efn.EfficientNetB0(input_shape=shape,weights='imagenet',include_top=False),\n",
    "        'b1': efn.EfficientNetB1(input_shape=shape,weights=None,include_top=False),\n",
    "        'b2': efn.EfficientNetB2(input_shape=shape,weights=None,include_top=False),\n",
    "        'b3': efn.EfficientNetB3(input_shape=shape,weights=None,include_top=False),\n",
    "        'b4': efn.EfficientNetB4(input_shape=shape,weights=None,include_top=False),\n",
    "        'b5': efn.EfficientNetB5(input_shape=shape,weights=None,include_top=False),\n",
    "        'b6': efn.EfficientNetB6(input_shape=shape,weights=None,include_top=False),\n",
    "        'b7': efn.EfficientNetB7(input_shape=shape,weights=None,include_top=False),\n",
    "        #'RNet50': resnet50.ResNet50(input_shape=shape,weights=None,include_top=False),\n",
    "        #'V16': vgg16.VGG16(input_shape=shape,weights=None,include_top=False)\n",
    "    }\n",
    "    return models_dict[model]\n",
    "\n",
    "def build_model(shape=(512, 512, 3), model_class=None):\n",
    "    inp = Input(shape=shape)\n",
    "    base = get_model(model_class, shape)\n",
    "    x = base(inp)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    inp2 = Input(shape=(4,)) # indicates that the expected input will be batches of 4-dimensional vectors\n",
    "    x2 = GaussianNoise(0.2)(inp2) # to mitigate overfitting\n",
    "    x = Concatenate()([x, x2]) \n",
    "    x = Dropout(0.5)(x) \n",
    "    x = Dense(1)(x)\n",
    "    model = Model([inp, inp2] , x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_101 (InputLayer)         [(None, 512, 512, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " efficientnet-b1 (Functional)   (None, 16, 16, 1280  6575232     ['input_101[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " input_110 (InputLayer)         [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " global_average_pooling2d_10 (G  (None, 1280)        0           ['efficientnet-b1[0][0]']        \n",
      " lobalAveragePooling2D)                                                                           \n",
      "                                                                                                  \n",
      " gaussian_noise_10 (GaussianNoi  (None, 4)           0           ['input_110[0][0]']              \n",
      " se)                                                                                              \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 1284)         0           ['global_average_pooling2d_10[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'gaussian_noise_10[0][0]']      \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 1284)         0           ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 1)            1285        ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,576,517\n",
      "Trainable params: 6,514,469\n",
      "Non-trainable params: 62,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MODEL_CLASS = 'b1'\n",
    "base_model = build_model(shape=(512, 512, 3), model_class=MODEL_CLASS)\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 100\n",
    "LR = 0.001\n",
    "SAVE_BEST = True\n",
    "tr_p, vl_p = train_test_split(P, shuffle=True, train_size= 0.8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000014F9D4C0318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000014F9D4C0318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9229WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000014F811F4F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000014F811F4F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 4.30482, saving model to b1_150_epochs.h5\n",
      "32/32 [==============================] - 117s 3s/step - loss: 4.9229 - val_loss: 4.3048 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.1297\n",
      "Epoch 2: val_loss did not improve from 4.30482\n",
      "32/32 [==============================] - 101s 3s/step - loss: 5.1297 - val_loss: 9.3438 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2788\n",
      "Epoch 3: val_loss improved from 4.30482 to 4.24683, saving model to b1_150_epochs.h5\n",
      "32/32 [==============================] - 102s 3s/step - loss: 4.2788 - val_loss: 4.2468 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0126\n",
      "Epoch 4: val_loss improved from 4.24683 to 4.14559, saving model to b1_150_epochs.h5\n",
      "32/32 [==============================] - 102s 3s/step - loss: 4.0126 - val_loss: 4.1456 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3889\n",
      "Epoch 5: val_loss did not improve from 4.14559\n",
      "32/32 [==============================] - 101s 3s/step - loss: 4.3889 - val_loss: 4.3781 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8775\n",
      "Epoch 6: val_loss did not improve from 4.14559\n",
      "32/32 [==============================] - 101s 3s/step - loss: 4.8775 - val_loss: 11.5987 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7644\n",
      "Epoch 7: val_loss did not improve from 4.14559\n",
      "32/32 [==============================] - 102s 3s/step - loss: 4.7644 - val_loss: 24.7704 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9031\n",
      "Epoch 8: val_loss did not improve from 4.14559\n",
      "32/32 [==============================] - 102s 3s/step - loss: 3.9031 - val_loss: 5.5936 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3134\n",
      "Epoch 9: val_loss did not improve from 4.14559\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "32/32 [==============================] - 102s 3s/step - loss: 4.3134 - val_loss: 9.9667 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7825\n",
      "Epoch 10: val_loss did not improve from 4.14559\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.7825 - val_loss: 4.2753 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3043\n",
      "Epoch 11: val_loss improved from 4.14559 to 4.06588, saving model to b1_150_epochs.h5\n",
      "32/32 [==============================] - 101s 3s/step - loss: 4.3043 - val_loss: 4.0659 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5535\n",
      "Epoch 12: val_loss did not improve from 4.06588\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.5535 - val_loss: 4.5170 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7629\n",
      "Epoch 13: val_loss improved from 4.06588 to 3.81294, saving model to b1_150_epochs.h5\n",
      "32/32 [==============================] - 101s 3s/step - loss: 3.7629 - val_loss: 3.8129 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2096\n",
      "Epoch 14: val_loss did not improve from 3.81294\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.2096 - val_loss: 4.4175 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8334\n",
      "Epoch 15: val_loss did not improve from 3.81294\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.8334 - val_loss: 4.3673 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0394\n",
      "Epoch 16: val_loss did not improve from 3.81294\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.0394 - val_loss: 4.5665 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4356\n",
      "Epoch 17: val_loss did not improve from 3.81294\n",
      "32/32 [==============================] - 101s 3s/step - loss: 4.4356 - val_loss: 4.5136 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2370\n",
      "Epoch 18: val_loss did not improve from 3.81294\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "32/32 [==============================] - 101s 3s/step - loss: 4.2370 - val_loss: 4.6745 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5663\n",
      "Epoch 19: val_loss did not improve from 3.81294\n",
      "32/32 [==============================] - 101s 3s/step - loss: 4.5663 - val_loss: 4.3810 - lr: 2.5000e-04\n",
      "Epoch 20/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4228\n",
      "Epoch 20: val_loss improved from 3.81294 to 3.73320, saving model to b1_150_epochs.h5\n",
      "32/32 [==============================] - 101s 3s/step - loss: 4.4228 - val_loss: 3.7332 - lr: 2.5000e-04\n",
      "Epoch 21/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.3694\n",
      "Epoch 21: val_loss did not improve from 3.73320\n",
      "32/32 [==============================] - 102s 3s/step - loss: 3.3694 - val_loss: 4.0906 - lr: 2.5000e-04\n",
      "Epoch 22/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6103\n",
      "Epoch 22: val_loss did not improve from 3.73320\n",
      "32/32 [==============================] - 102s 3s/step - loss: 4.6103 - val_loss: 4.3044 - lr: 2.5000e-04\n",
      "Epoch 23/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2394\n",
      "Epoch 23: val_loss did not improve from 3.73320\n",
      "32/32 [==============================] - 103s 3s/step - loss: 4.2394 - val_loss: 4.7650 - lr: 2.5000e-04\n",
      "Epoch 24/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7590\n",
      "Epoch 24: val_loss did not improve from 3.73320\n",
      "32/32 [==============================] - 104s 3s/step - loss: 4.7590 - val_loss: 4.7668 - lr: 2.5000e-04\n",
      "Epoch 25/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0682\n",
      "Epoch 25: val_loss did not improve from 3.73320\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "32/32 [==============================] - 102s 3s/step - loss: 4.0682 - val_loss: 4.7658 - lr: 2.5000e-04\n",
      "Epoch 26/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8787\n",
      "Epoch 26: val_loss did not improve from 3.73320\n",
      "32/32 [==============================] - 103s 3s/step - loss: 4.8787 - val_loss: 4.0187 - lr: 1.2500e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2575\n",
      "Epoch 27: val_loss did not improve from 3.73320\n",
      "32/32 [==============================] - 103s 3s/step - loss: 4.2575 - val_loss: 4.7137 - lr: 1.2500e-04\n",
      "Epoch 28/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2395\n",
      "Epoch 28: val_loss did not improve from 3.73320\n",
      "32/32 [==============================] - 109s 3s/step - loss: 4.2395 - val_loss: 4.5860 - lr: 1.2500e-04\n",
      "Epoch 29/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0341\n",
      "Epoch 29: val_loss did not improve from 3.73320\n",
      "32/32 [==============================] - 140s 4s/step - loss: 4.0341 - val_loss: 4.2050 - lr: 1.2500e-04\n",
      "Epoch 30/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6239\n",
      "Epoch 30: val_loss did not improve from 3.73320\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "32/32 [==============================] - 101s 3s/step - loss: 4.6239 - val_loss: 3.9206 - lr: 1.2500e-04\n",
      "Epoch 31/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0327\n",
      "Epoch 31: val_loss did not improve from 3.73320\n",
      "32/32 [==============================] - 101s 3s/step - loss: 4.0327 - val_loss: 3.9905 - lr: 6.2500e-05\n",
      "Epoch 32/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7575\n",
      "Epoch 32: val_loss did not improve from 3.73320\n",
      "32/32 [==============================] - 120s 4s/step - loss: 4.7575 - val_loss: 4.9668 - lr: 6.2500e-05\n",
      "Epoch 33/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3229\n",
      "Epoch 33: val_loss did not improve from 3.73320\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.3229 - val_loss: 3.9333 - lr: 6.2500e-05\n",
      "Epoch 34/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7923\n",
      "Epoch 34: val_loss did not improve from 3.73320\n",
      "32/32 [==============================] - 99s 3s/step - loss: 4.7923 - val_loss: 4.0401 - lr: 6.2500e-05\n",
      "Epoch 35/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.2674\n",
      "Epoch 35: val_loss did not improve from 3.73320\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "32/32 [==============================] - 100s 3s/step - loss: 5.2674 - val_loss: 4.7540 - lr: 6.2500e-05\n",
      "Epoch 36/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4822\n",
      "Epoch 36: val_loss did not improve from 3.73320\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.4822 - val_loss: 3.8429 - lr: 3.1250e-05\n",
      "Epoch 37/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9018\n",
      "Epoch 37: val_loss improved from 3.73320 to 3.65919, saving model to b1_150_epochs.h5\n",
      "32/32 [==============================] - 100s 3s/step - loss: 3.9018 - val_loss: 3.6592 - lr: 3.1250e-05\n",
      "Epoch 38/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1576\n",
      "Epoch 38: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.1576 - val_loss: 4.3915 - lr: 3.1250e-05\n",
      "Epoch 39/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2816\n",
      "Epoch 39: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.2816 - val_loss: 4.7845 - lr: 3.1250e-05\n",
      "Epoch 40/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1357\n",
      "Epoch 40: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.1357 - val_loss: 4.0132 - lr: 3.1250e-05\n",
      "Epoch 41/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6740\n",
      "Epoch 41: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.6740 - val_loss: 3.9217 - lr: 3.1250e-05\n",
      "Epoch 42/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4965\n",
      "Epoch 42: val_loss did not improve from 3.65919\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "32/32 [==============================] - 101s 3s/step - loss: 4.4965 - val_loss: 4.0729 - lr: 3.1250e-05\n",
      "Epoch 43/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9021\n",
      "Epoch 43: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.9021 - val_loss: 4.2388 - lr: 1.5625e-05\n",
      "Epoch 44/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.6610\n",
      "Epoch 44: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 99s 3s/step - loss: 5.6610 - val_loss: 3.9805 - lr: 1.5625e-05\n",
      "Epoch 45/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0936\n",
      "Epoch 45: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.0936 - val_loss: 3.8800 - lr: 1.5625e-05\n",
      "Epoch 46/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7687\n",
      "Epoch 46: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.7687 - val_loss: 3.8687 - lr: 1.5625e-05\n",
      "Epoch 47/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0647\n",
      "Epoch 47: val_loss did not improve from 3.65919\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.0647 - val_loss: 4.2830 - lr: 1.5625e-05\n",
      "Epoch 48/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2326\n",
      "Epoch 48: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.2326 - val_loss: 4.8242 - lr: 7.8125e-06\n",
      "Epoch 49/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8812\n",
      "Epoch 49: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 99s 3s/step - loss: 3.8812 - val_loss: 4.7569 - lr: 7.8125e-06\n",
      "Epoch 50/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8973\n",
      "Epoch 50: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 99s 3s/step - loss: 4.8973 - val_loss: 4.4247 - lr: 7.8125e-06\n",
      "Epoch 51/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2497\n",
      "Epoch 51: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 99s 3s/step - loss: 4.2497 - val_loss: 4.2082 - lr: 7.8125e-06\n",
      "Epoch 52/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0856\n",
      "Epoch 52: val_loss did not improve from 3.65919\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "32/32 [==============================] - 99s 3s/step - loss: 4.0856 - val_loss: 4.8082 - lr: 7.8125e-06\n",
      "Epoch 53/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2428\n",
      "Epoch 53: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 99s 3s/step - loss: 4.2428 - val_loss: 4.0556 - lr: 3.9063e-06\n",
      "Epoch 54/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4470\n",
      "Epoch 54: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 98s 3s/step - loss: 4.4470 - val_loss: 4.1296 - lr: 3.9063e-06\n",
      "Epoch 55/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8235\n",
      "Epoch 55: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 99s 3s/step - loss: 4.8235 - val_loss: 4.3784 - lr: 3.9063e-06\n",
      "Epoch 56/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1965\n",
      "Epoch 56: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 99s 3s/step - loss: 4.1965 - val_loss: 3.9745 - lr: 3.9063e-06\n",
      "Epoch 57/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4155\n",
      "Epoch 57: val_loss did not improve from 3.65919\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "32/32 [==============================] - 98s 3s/step - loss: 4.4155 - val_loss: 3.7698 - lr: 3.9063e-06\n",
      "Epoch 58/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.4359\n",
      "Epoch 58: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 98s 3s/step - loss: 5.4359 - val_loss: 4.9100 - lr: 1.9531e-06\n",
      "Epoch 59/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5516\n",
      "Epoch 59: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 98s 3s/step - loss: 4.5516 - val_loss: 4.0006 - lr: 1.9531e-06\n",
      "Epoch 60/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9127\n",
      "Epoch 60: val_loss did not improve from 3.65919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 98s 3s/step - loss: 3.9127 - val_loss: 4.1942 - lr: 1.9531e-06\n",
      "Epoch 61/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8191\n",
      "Epoch 61: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 98s 3s/step - loss: 3.8191 - val_loss: 5.4372 - lr: 1.9531e-06\n",
      "Epoch 62/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9776\n",
      "Epoch 62: val_loss did not improve from 3.65919\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "32/32 [==============================] - 98s 3s/step - loss: 3.9776 - val_loss: 4.3971 - lr: 1.9531e-06\n",
      "Epoch 63/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6422\n",
      "Epoch 63: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 98s 3s/step - loss: 3.6422 - val_loss: 4.4038 - lr: 9.7656e-07\n",
      "Epoch 64/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5134\n",
      "Epoch 64: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 98s 3s/step - loss: 4.5134 - val_loss: 4.4756 - lr: 9.7656e-07\n",
      "Epoch 65/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1712\n",
      "Epoch 65: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 98s 3s/step - loss: 4.1712 - val_loss: 5.0829 - lr: 9.7656e-07\n",
      "Epoch 66/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0839\n",
      "Epoch 66: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 98s 3s/step - loss: 4.0839 - val_loss: 4.0093 - lr: 9.7656e-07\n",
      "Epoch 67/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5406\n",
      "Epoch 67: val_loss did not improve from 3.65919\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "32/32 [==============================] - 98s 3s/step - loss: 4.5406 - val_loss: 3.9094 - lr: 9.7656e-07\n",
      "Epoch 68/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3870\n",
      "Epoch 68: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 98s 3s/step - loss: 4.3870 - val_loss: 4.4061 - lr: 4.8828e-07\n",
      "Epoch 69/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5294\n",
      "Epoch 69: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 99s 3s/step - loss: 4.5294 - val_loss: 4.3440 - lr: 4.8828e-07\n",
      "Epoch 70/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2708\n",
      "Epoch 70: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 99s 3s/step - loss: 4.2708 - val_loss: 4.1517 - lr: 4.8828e-07\n",
      "Epoch 71/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8271\n",
      "Epoch 71: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 98s 3s/step - loss: 3.8271 - val_loss: 4.1525 - lr: 4.8828e-07\n",
      "Epoch 72/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8572\n",
      "Epoch 72: val_loss did not improve from 3.65919\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "32/32 [==============================] - 98s 3s/step - loss: 3.8572 - val_loss: 4.6161 - lr: 4.8828e-07\n",
      "Epoch 73/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4656\n",
      "Epoch 73: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 98s 3s/step - loss: 4.4656 - val_loss: 4.3212 - lr: 2.4414e-07\n",
      "Epoch 74/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5049\n",
      "Epoch 74: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 99s 3s/step - loss: 4.5049 - val_loss: 4.0564 - lr: 2.4414e-07\n",
      "Epoch 75/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7286\n",
      "Epoch 75: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 99s 3s/step - loss: 3.7286 - val_loss: 4.2663 - lr: 2.4414e-07\n",
      "Epoch 76/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3096\n",
      "Epoch 76: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 99s 3s/step - loss: 4.3096 - val_loss: 4.4586 - lr: 2.4414e-07\n",
      "Epoch 77/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.1931\n",
      "Epoch 77: val_loss did not improve from 3.65919\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "32/32 [==============================] - 99s 3s/step - loss: 5.1931 - val_loss: 3.8102 - lr: 2.4414e-07\n",
      "Epoch 78/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.1083\n",
      "Epoch 78: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 99s 3s/step - loss: 5.1083 - val_loss: 3.9167 - lr: 1.2207e-07\n",
      "Epoch 79/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6895\n",
      "Epoch 79: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 99s 3s/step - loss: 4.6895 - val_loss: 4.0801 - lr: 1.2207e-07\n",
      "Epoch 80/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3773\n",
      "Epoch 80: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 99s 3s/step - loss: 4.3773 - val_loss: 4.4620 - lr: 1.2207e-07\n",
      "Epoch 81/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8745\n",
      "Epoch 81: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 99s 3s/step - loss: 4.8745 - val_loss: 4.1971 - lr: 1.2207e-07\n",
      "Epoch 82/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4545\n",
      "Epoch 82: val_loss did not improve from 3.65919\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.4545 - val_loss: 4.9147 - lr: 1.2207e-07\n",
      "Epoch 83/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9787\n",
      "Epoch 83: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 100s 3s/step - loss: 3.9787 - val_loss: 4.8326 - lr: 6.1035e-08\n",
      "Epoch 84/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2011\n",
      "Epoch 84: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.2011 - val_loss: 4.5547 - lr: 6.1035e-08\n",
      "Epoch 85/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5356\n",
      "Epoch 85: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 101s 3s/step - loss: 4.5356 - val_loss: 4.7942 - lr: 6.1035e-08\n",
      "Epoch 86/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6399\n",
      "Epoch 86: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 100s 3s/step - loss: 4.6399 - val_loss: 4.6508 - lr: 6.1035e-08\n",
      "Epoch 87/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3250\n",
      "Epoch 87: val_loss did not improve from 3.65919\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "32/32 [==============================] - 101s 3s/step - loss: 4.3250 - val_loss: 4.1195 - lr: 6.1035e-08\n",
      "Epoch 88/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1429\n",
      "Epoch 88: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 101s 3s/step - loss: 4.1429 - val_loss: 4.3154 - lr: 3.0518e-08\n",
      "Epoch 89/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4172\n",
      "Epoch 89: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 101s 3s/step - loss: 4.4172 - val_loss: 3.7368 - lr: 3.0518e-08\n",
      "Epoch 90/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4492\n",
      "Epoch 90: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 101s 3s/step - loss: 4.4492 - val_loss: 4.7973 - lr: 3.0518e-08\n",
      "Epoch 91/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2740\n",
      "Epoch 91: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 101s 3s/step - loss: 4.2740 - val_loss: 4.7979 - lr: 3.0518e-08\n",
      "Epoch 92/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3492\n",
      "Epoch 92: val_loss did not improve from 3.65919\n",
      "\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "32/32 [==============================] - 101s 3s/step - loss: 4.3492 - val_loss: 4.8039 - lr: 3.0518e-08\n",
      "Epoch 93/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7977\n",
      "Epoch 93: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 101s 3s/step - loss: 3.7977 - val_loss: 4.3059 - lr: 1.5259e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0841\n",
      "Epoch 94: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 101s 3s/step - loss: 4.0841 - val_loss: 4.4391 - lr: 1.5259e-08\n",
      "Epoch 95/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9852\n",
      "Epoch 95: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 101s 3s/step - loss: 3.9852 - val_loss: 4.3481 - lr: 1.5259e-08\n",
      "Epoch 96/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1047\n",
      "Epoch 96: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 102s 3s/step - loss: 4.1047 - val_loss: 4.3367 - lr: 1.5259e-08\n",
      "Epoch 97/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3212\n",
      "Epoch 97: val_loss did not improve from 3.65919\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 1e-08.\n",
      "32/32 [==============================] - 102s 3s/step - loss: 4.3212 - val_loss: 4.8511 - lr: 1.5259e-08\n",
      "Epoch 98/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2981\n",
      "Epoch 98: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 102s 3s/step - loss: 4.2981 - val_loss: 4.1058 - lr: 1.0000e-08\n",
      "Epoch 99/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3487\n",
      "Epoch 99: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 102s 3s/step - loss: 4.3487 - val_loss: 4.8889 - lr: 1.0000e-08\n",
      "Epoch 100/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9201\n",
      "Epoch 100: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 101s 3s/step - loss: 3.9201 - val_loss: 3.8469 - lr: 1.0000e-08\n",
      "Epoch 101/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9464\n",
      "Epoch 101: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 102s 3s/step - loss: 3.9464 - val_loss: 3.8164 - lr: 1.0000e-08\n",
      "Epoch 102/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1957\n",
      "Epoch 102: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 102s 3s/step - loss: 4.1957 - val_loss: 4.2459 - lr: 1.0000e-08\n",
      "Epoch 103/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9633\n",
      "Epoch 103: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 102s 3s/step - loss: 4.9633 - val_loss: 4.1076 - lr: 1.0000e-08\n",
      "Epoch 104/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8081\n",
      "Epoch 104: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 102s 3s/step - loss: 3.8081 - val_loss: 4.1286 - lr: 1.0000e-08\n",
      "Epoch 105/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4188\n",
      "Epoch 105: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 102s 3s/step - loss: 4.4188 - val_loss: 3.8947 - lr: 1.0000e-08\n",
      "Epoch 106/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9331\n",
      "Epoch 106: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 102s 3s/step - loss: 3.9331 - val_loss: 3.8544 - lr: 1.0000e-08\n",
      "Epoch 107/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7183\n",
      "Epoch 107: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 102s 3s/step - loss: 3.7183 - val_loss: 4.5812 - lr: 1.0000e-08\n",
      "Epoch 108/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8276\n",
      "Epoch 108: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 102s 3s/step - loss: 4.8276 - val_loss: 4.5118 - lr: 1.0000e-08\n",
      "Epoch 109/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0303\n",
      "Epoch 109: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 102s 3s/step - loss: 4.0303 - val_loss: 3.9331 - lr: 1.0000e-08\n",
      "Epoch 110/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4113\n",
      "Epoch 110: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 102s 3s/step - loss: 4.4113 - val_loss: 3.9609 - lr: 1.0000e-08\n",
      "Epoch 111/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6346\n",
      "Epoch 111: val_loss did not improve from 3.65919\n",
      "32/32 [==============================] - 103s 3s/step - loss: 4.6346 - val_loss: 4.1774 - lr: 1.0000e-08\n",
      "Epoch 112/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8475\n",
      "Epoch 112: val_loss improved from 3.65919 to 3.57325, saving model to b1_150_epochs.h5\n",
      "32/32 [==============================] - 103s 3s/step - loss: 3.8475 - val_loss: 3.5732 - lr: 1.0000e-08\n",
      "Epoch 113/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2559\n",
      "Epoch 113: val_loss did not improve from 3.57325\n",
      "32/32 [==============================] - 103s 3s/step - loss: 4.2559 - val_loss: 3.7501 - lr: 1.0000e-08\n",
      "Epoch 114/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6930\n",
      "Epoch 114: val_loss did not improve from 3.57325\n",
      "32/32 [==============================] - 103s 3s/step - loss: 4.6930 - val_loss: 4.1401 - lr: 1.0000e-08\n",
      "Epoch 115/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2657\n",
      "Epoch 115: val_loss did not improve from 3.57325\n",
      "32/32 [==============================] - 104s 3s/step - loss: 4.2657 - val_loss: 3.8613 - lr: 1.0000e-08\n",
      "Epoch 116/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1946\n",
      "Epoch 116: val_loss did not improve from 3.57325\n",
      "32/32 [==============================] - 103s 3s/step - loss: 4.1946 - val_loss: 3.8462 - lr: 1.0000e-08\n",
      "Epoch 117/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0385\n",
      "Epoch 117: val_loss did not improve from 3.57325\n",
      "32/32 [==============================] - 103s 3s/step - loss: 4.0385 - val_loss: 5.2866 - lr: 1.0000e-08\n",
      "Epoch 118/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0155\n",
      "Epoch 118: val_loss did not improve from 3.57325\n",
      "32/32 [==============================] - 103s 3s/step - loss: 4.0155 - val_loss: 3.7124 - lr: 1.0000e-08\n",
      "Epoch 119/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6004\n",
      "Epoch 119: val_loss did not improve from 3.57325\n",
      "32/32 [==============================] - 103s 3s/step - loss: 3.6004 - val_loss: 4.4111 - lr: 1.0000e-08\n",
      "Epoch 120/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1278\n",
      "Epoch 120: val_loss did not improve from 3.57325\n",
      "32/32 [==============================] - 103s 3s/step - loss: 4.1278 - val_loss: 3.8944 - lr: 1.0000e-08\n",
      "Epoch 121/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9234\n",
      "Epoch 121: val_loss did not improve from 3.57325\n",
      "32/32 [==============================] - 103s 3s/step - loss: 3.9234 - val_loss: 3.8208 - lr: 1.0000e-08\n",
      "Epoch 122/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1665\n",
      "Epoch 122: val_loss did not improve from 3.57325\n",
      "32/32 [==============================] - 103s 3s/step - loss: 4.1665 - val_loss: 4.5684 - lr: 1.0000e-08\n",
      "Epoch 123/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4248\n",
      "Epoch 123: val_loss did not improve from 3.57325\n",
      "32/32 [==============================] - 104s 3s/step - loss: 4.4248 - val_loss: 4.2988 - lr: 1.0000e-08\n",
      "Epoch 124/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7766\n",
      "Epoch 124: val_loss improved from 3.57325 to 3.52827, saving model to b1_150_epochs.h5\n",
      "32/32 [==============================] - 105s 3s/step - loss: 3.7766 - val_loss: 3.5283 - lr: 1.0000e-08\n",
      "Epoch 125/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8487\n",
      "Epoch 125: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 104s 3s/step - loss: 4.8487 - val_loss: 4.1337 - lr: 1.0000e-08\n",
      "Epoch 126/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5103\n",
      "Epoch 126: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 104s 3s/step - loss: 4.5103 - val_loss: 4.2089 - lr: 1.0000e-08\n",
      "Epoch 127/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5725\n",
      "Epoch 127: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 104s 3s/step - loss: 4.5725 - val_loss: 4.1071 - lr: 1.0000e-08\n",
      "Epoch 128/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1895\n",
      "Epoch 128: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 105s 3s/step - loss: 4.1895 - val_loss: 4.1164 - lr: 1.0000e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7533\n",
      "Epoch 129: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.7533 - val_loss: 4.7293 - lr: 1.0000e-08\n",
      "Epoch 130/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4013\n",
      "Epoch 130: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.4013 - val_loss: 4.3871 - lr: 1.0000e-08\n",
      "Epoch 131/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9391\n",
      "Epoch 131: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.9391 - val_loss: 4.3349 - lr: 1.0000e-08\n",
      "Epoch 132/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6170\n",
      "Epoch 132: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.6170 - val_loss: 4.4253 - lr: 1.0000e-08\n",
      "Epoch 133/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1339\n",
      "Epoch 133: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.1339 - val_loss: 4.9340 - lr: 1.0000e-08\n",
      "Epoch 134/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0556\n",
      "Epoch 134: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.0556 - val_loss: 4.4139 - lr: 1.0000e-08\n",
      "Epoch 135/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0086\n",
      "Epoch 135: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.0086 - val_loss: 4.0051 - lr: 1.0000e-08\n",
      "Epoch 136/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0309\n",
      "Epoch 136: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.0309 - val_loss: 4.0878 - lr: 1.0000e-08\n",
      "Epoch 137/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6979\n",
      "Epoch 137: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.6979 - val_loss: 4.7554 - lr: 1.0000e-08\n",
      "Epoch 138/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1198\n",
      "Epoch 138: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.1198 - val_loss: 4.6752 - lr: 1.0000e-08\n",
      "Epoch 139/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5543\n",
      "Epoch 139: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.5543 - val_loss: 5.4915 - lr: 1.0000e-08\n",
      "Epoch 140/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1657\n",
      "Epoch 140: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.1657 - val_loss: 4.2804 - lr: 1.0000e-08\n",
      "Epoch 141/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0993\n",
      "Epoch 141: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.0993 - val_loss: 4.1867 - lr: 1.0000e-08\n",
      "Epoch 142/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7963\n",
      "Epoch 142: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.7963 - val_loss: 3.8684 - lr: 1.0000e-08\n",
      "Epoch 143/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8360\n",
      "Epoch 143: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 106s 3s/step - loss: 3.8360 - val_loss: 3.9150 - lr: 1.0000e-08\n",
      "Epoch 144/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6663\n",
      "Epoch 144: val_loss did not improve from 3.52827\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.6663 - val_loss: 4.1739 - lr: 1.0000e-08\n",
      "Epoch 145/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2846\n",
      "Epoch 145: val_loss improved from 3.52827 to 3.38566, saving model to b1_150_epochs.h5\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.2846 - val_loss: 3.3857 - lr: 1.0000e-08\n",
      "Epoch 146/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3252\n",
      "Epoch 146: val_loss did not improve from 3.38566\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.3252 - val_loss: 3.3881 - lr: 1.0000e-08\n",
      "Epoch 147/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.0342\n",
      "Epoch 147: val_loss did not improve from 3.38566\n",
      "32/32 [==============================] - 106s 3s/step - loss: 5.0342 - val_loss: 4.1932 - lr: 1.0000e-08\n",
      "Epoch 148/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0677\n",
      "Epoch 148: val_loss did not improve from 3.38566\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.0677 - val_loss: 4.6936 - lr: 1.0000e-08\n",
      "Epoch 149/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5988\n",
      "Epoch 149: val_loss did not improve from 3.38566\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.5988 - val_loss: 4.7607 - lr: 1.0000e-08\n",
      "Epoch 150/150\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2689\n",
      "Epoch 150: val_loss did not improve from 3.38566\n",
      "32/32 [==============================] - 106s 3s/step - loss: 4.2689 - val_loss: 3.9759 - lr: 1.0000e-08\n",
      "Training Complete!!!\n"
     ]
    }
   ],
   "source": [
    "P = np.array(P)\n",
    "subs = []\n",
    "folds_history = []\n",
    "\n",
    "\"\"\"\n",
    "er = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",min_delta=1e-3,patience=15,verbose=1,mode=\"auto\",baseline=None,\n",
    "      restore_best_weights=True,) #Stop training when a monitored metric has stopped improving.\n",
    "\"\"\"\n",
    "\n",
    "cpt = tf.keras.callbacks.ModelCheckpoint(filepath=f'{MODEL_CLASS}_{EPOCHS}_epochs.h5',monitor='val_loss',verbose=1, \n",
    "    save_best_only=SAVE_BEST,mode='auto') #to save model or weights in a checkpoint file at lowest validation loss\n",
    "\n",
    "rlp = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.5,patience=5, verbose=1, min_lr=1e-8) \n",
    "     #Reduce learning rate when a metric has stopped improving.\n",
    "     # if improvement stops, after 5 epochs learning rate will be reduced\n",
    "\n",
    "model = build_model(model_class=MODEL_CLASS)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR), loss=\"mae\") \n",
    "history = model.fit(IGenerator(keys=P, a = A, tab = TAB, batch_size=BATCH_SIZE), \n",
    "                    steps_per_epoch = 32,\n",
    "                    validation_data=IGenerator(keys=P, a = A, tab = TAB, batch_size=BATCH_SIZE),\n",
    "                    validation_steps = 32, \n",
    "                    callbacks = [cpt, rlp], \n",
    "                    epochs=EPOCHS)\n",
    "folds_history.append(history.history)\n",
    "print('Training Complete!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "# save to json:  \n",
    "hist_json_file = './Saved_Models/EffNet_b1_history_100_epoch_imagenet.json' \n",
    "with open(hist_json_file, mode='w') as f:\n",
    "    hist_df.to_json(f)\n",
    "\n",
    "# or save to csv: \n",
    "hist_csv_file = './Saved_Models/EffNet_b1_history_100_epoch_imagenet.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = './Dataset/train/B1'\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(EPOCHS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs_range, loss, 'b1', label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(result_dir + 'b1_100_epoch_loss_imagenet.png')\n",
    "plt.savefig(result_dir + 'b1_100_epoch_loss_imagenet.pdf', dpi=100)\n",
    "tikzplotlib.save(result_dir + 'b1_100_epoch_loss_imagenet.tex')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.85):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_range = range(20,EPOCHS)\n",
    "plt.plot(epochs_range,\n",
    "         smooth_curve(loss[20:]), 'b1', label='Training loss')\n",
    "plt.plot(epochs_range,\n",
    "         smooth_curve(val_loss[20:]), 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.savefig(result_dir + 'b1_100_epoch_loss_smooth_imagenet.png')\n",
    "plt.savefig(result_dir + 'b1_100_epoch_loss_smooth_imagenet.pdf', dpi=100)\n",
    "tikzplotlib.save(result_dir + 'b1_100_epoch_loss_smooth_imagenet.tex')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(fvc_true, fvc_pred, sigma):\n",
    "    sigma_clip = np.maximum(sigma, 70)\n",
    "    delta = np.abs(fvc_true - fvc_pred)\n",
    "    delta = np.minimum(delta, 1000)\n",
    "    sq2 = np.sqrt(2)\n",
    "    metric = (delta / sigma_clip)*sq2 + np.log(sigma_clip* sq2)\n",
    "    return np.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd09f528c6ea4ec388dc7104f3fcadf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.75 GiB for an array with shape (469, 512, 512, 3) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4820\\1213214032.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0m_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtab\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mexpand_dims\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\numpy\\lib\\shape_base.py\u001b[0m in \u001b[0;36mexpand_dims\u001b[1;34m(a, axis)\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.75 GiB for an array with shape (469, 512, 512, 3) and data type float64"
     ]
    }
   ],
   "source": [
    "metric = []\n",
    "for q in tqdm(range(1, 5)):\n",
    "    m = []\n",
    "    for p in vl_p:\n",
    "        x = [] \n",
    "        tab = [] \n",
    "        \n",
    "        if p in ['ID00011637202177653955184', 'ID00052637202186188008618']:\n",
    "            continue\n",
    "        for i in os.listdir(f'./Dataset/train/{p}/'):\n",
    "            x.append(get_img(f'./Dataset/train/{p}/{i}')) \n",
    "            tab.append(get_tab(train.loc[train.Patient == p, :])) \n",
    "        tab = np.array(tab) \n",
    "    \n",
    "        x = np.expand_dims(x, axis=-1) \n",
    "        _a = model.predict([x, tab]) \n",
    "        a = np.quantile(_a, q / 10)\n",
    "        \n",
    "        percent_true = train.Percent.values[train.Patient == p]\n",
    "        fvc_true = train.FVC.values[train.Patient == p]\n",
    "        weeks_true = train.Weeks.values[train.Patient == p]\n",
    "        \n",
    "        fvc = a * (weeks_true - weeks_true[0]) + fvc_true[0]\n",
    "        percent = percent_true[0] - a * abs(weeks_true - weeks_true[0])\n",
    "        m.append(score(fvc_true, fvc, percent))\n",
    "    print(np.mean(m))\n",
    "    metric.append(np.mean(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = (np.argmin(metric) + 1)/ 2\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./Dataset//sample_submission.csv') \n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./Dataset//test.csv') \n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_test, B_test, P_test, W, FVC, STD, WEEK = {},{},{},{},{},{},{} \n",
    "\n",
    "for p in test.Patient.unique():\n",
    "    x = [] \n",
    "    tab = [] \n",
    "    for i in os.listdir(f'./Dataset//test/{p}/'):\n",
    "        x.append(get_img(f'./Dataset//test/{p}/{i}')) \n",
    "        tab.append(get_tab(test.loc[test.Patient == p, :])) \n",
    "    tab = np.array(tab) \n",
    "            \n",
    "    x = np.expand_dims(x, axis=-1) \n",
    "    _a = model.predict([x, tab]) \n",
    "    a = np.quantile(_a, q)\n",
    "    A_test[p] = a\n",
    "    B_test[p] = test.FVC.values[test.Patient == p] - a*test.Weeks.values[test.Patient == p]\n",
    "    P_test[p] = test.Percent.values[test.Patient == p] \n",
    "    WEEK[p] = test.Weeks.values[test.Patient == p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'Patient_Week'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4820\\848746774.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPatient_Week\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfvc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mB_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5460\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5461\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5462\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5464\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Patient_Week'"
     ]
    }
   ],
   "source": [
    "for k in sub.Patient_Week.values:\n",
    "    p, w = k.split('_')\n",
    "    w = int(w) \n",
    "    \n",
    "    fvc = A_test[p] * w + B_test[p]\n",
    "    sub.loc[sub.Patient_Week == k, 'FVC'] = fvc\n",
    "    sub.loc[sub.Patient_Week == k, 'Confidence'] = (\n",
    "        P_test[p] - A_test[p] * abs(WEEK[p] - w) \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>Weeks</th>\n",
       "      <th>FVC</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SmokingStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>ID00426637202313170790466</td>\n",
       "      <td>0</td>\n",
       "      <td>2925</td>\n",
       "      <td>71.824968</td>\n",
       "      <td>73</td>\n",
       "      <td>Male</td>\n",
       "      <td>Never smoked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>ID00426637202313170790466</td>\n",
       "      <td>7</td>\n",
       "      <td>2903</td>\n",
       "      <td>71.284746</td>\n",
       "      <td>73</td>\n",
       "      <td>Male</td>\n",
       "      <td>Never smoked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>ID00426637202313170790466</td>\n",
       "      <td>9</td>\n",
       "      <td>2916</td>\n",
       "      <td>71.603968</td>\n",
       "      <td>73</td>\n",
       "      <td>Male</td>\n",
       "      <td>Never smoked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>ID00426637202313170790466</td>\n",
       "      <td>11</td>\n",
       "      <td>2976</td>\n",
       "      <td>73.077301</td>\n",
       "      <td>73</td>\n",
       "      <td>Male</td>\n",
       "      <td>Never smoked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>ID00426637202313170790466</td>\n",
       "      <td>13</td>\n",
       "      <td>2712</td>\n",
       "      <td>66.594637</td>\n",
       "      <td>73</td>\n",
       "      <td>Male</td>\n",
       "      <td>Never smoked</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Patient  Weeks   FVC    Percent  Age   Sex  \\\n",
       "1540  ID00426637202313170790466      0  2925  71.824968   73  Male   \n",
       "1541  ID00426637202313170790466      7  2903  71.284746   73  Male   \n",
       "1542  ID00426637202313170790466      9  2916  71.603968   73  Male   \n",
       "1543  ID00426637202313170790466     11  2976  73.077301   73  Male   \n",
       "1544  ID00426637202313170790466     13  2712  66.594637   73  Male   \n",
       "\n",
       "     SmokingStatus  \n",
       "1540  Never smoked  \n",
       "1541  Never smoked  \n",
       "1542  Never smoked  \n",
       "1543  Never smoked  \n",
       "1544  Never smoked  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Patient_Week', 'Confidence'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4820\\840817070.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msub\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Patient_Week\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"FVC\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Confidence\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"submission.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3028\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3029\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3030\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3032\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1264\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1266\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1267\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1314\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1316\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{not_found} not in index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Patient_Week', 'Confidence'] not in index\""
     ]
    }
   ],
   "source": [
    "sub[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add infos\n"
     ]
    }
   ],
   "source": [
    "ROOT = \"./Dataset//\"\n",
    "BATCH_SIZE=128\n",
    "\n",
    "tr = pd.read_csv(f\"{ROOT}/train.csv\")\n",
    "tr.drop_duplicates(keep=False, inplace=True, subset=['Patient','Weeks'])\n",
    "chunk = pd.read_csv(f\"{ROOT}/test.csv\")\n",
    "\n",
    "print(\"add infos\")\n",
    "sub = pd.read_csv(f\"{ROOT}/sample_submission.csv\")\n",
    "sub['Patient'] = sub['Patient_Week'].apply(lambda x:x.split('_')[0])\n",
    "sub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\n",
    "sub =  sub[['Patient','Weeks','Confidence','Patient_Week']]\n",
    "sub = sub.merge(chunk.drop('Weeks', axis=1), on=\"Patient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['WHERE'] = 'train'\n",
    "chunk['WHERE'] = 'val'\n",
    "sub['WHERE'] = 'test'\n",
    "data = tr.append([chunk, sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1535, 8) (5, 8) (730, 10) (2270, 10)\n",
      "176 5 5 176\n"
     ]
    }
   ],
   "source": [
    "print(tr.shape, chunk.shape, sub.shape, data.shape)\n",
    "print(tr.Patient.nunique(), chunk.Patient.nunique(), sub.Patient.nunique(), \n",
    "      data.Patient.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['min_week'] = data['Weeks']\n",
    "data.loc[data.WHERE=='test','min_week'] = np.nan\n",
    "data['min_week'] = data.groupby('Patient')['min_week'].transform('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = data.loc[data.Weeks == data.min_week]\n",
    "base = base[['Patient','FVC']].copy()\n",
    "base.columns = ['Patient','min_FVC']\n",
    "base['nb'] = 1\n",
    "base['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\n",
    "base = base[base.nb==1]\n",
    "base.drop('nb', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(base, on='Patient', how='left')\n",
    "data['base_week'] = data['Weeks'] - data['min_week']\n",
    "del base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = ['Sex','SmokingStatus'] #,'Age'\n",
    "FE = []\n",
    "for col in COLS:\n",
    "    for mod in data[col].unique():\n",
    "        FE.append(mod)\n",
    "        data[mod] = (data[col] == mod).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['age'] = (data['Age'] - data['Age'].min() ) / ( data['Age'].max() - data['Age'].min() )\n",
    "data['BASE'] = (data['min_FVC'] - data['min_FVC'].min() ) / ( data['min_FVC'].max() - data['min_FVC'].min() )\n",
    "data['week'] = (data['base_week'] - data['base_week'].min() ) / ( data['base_week'].max() - data['base_week'].min() )\n",
    "data['percent'] = (data['Percent'] - data['Percent'].min() ) / ( data['Percent'].max() - data['Percent'].min() )\n",
    "FE += ['age','percent','week','BASE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = data.loc[data.WHERE=='train']\n",
    "chunk = data.loc[data.WHERE=='val']\n",
    "sub = data.loc[data.WHERE=='test']\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1535, 22), (5, 22), (730, 22))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.shape, chunk.shape, sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    tf.dtypes.cast(y_true, tf.float32)\n",
    "    tf.dtypes.cast(y_pred, tf.float32)\n",
    "    sigma = y_pred[:, 2] - y_pred[:, 0]\n",
    "    fvc_pred = y_pred[:, 1]\n",
    "    \n",
    "    sigma_clip = tf.maximum(sigma, C1)\n",
    "    delta = tf.abs(y_true[:, 0] - fvc_pred)\n",
    "    delta = tf.minimum(delta, C2)\n",
    "    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n",
    "    metric = (delta / sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n",
    "    return K.mean(metric)\n",
    "\n",
    "def qloss(y_true, y_pred):\n",
    "    qs = [0.2, 0.50, 0.8]\n",
    "    q = tf.constant(np.array([qs]), dtype=tf.float32)\n",
    "    e = y_true - y_pred\n",
    "    v = tf.maximum(q*e, (q-1)*e)\n",
    "    return K.mean(v)\n",
    "\n",
    "def mloss(_lambda):\n",
    "    def loss(y_true, y_pred):\n",
    "        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def laplace_log_likelihood(actual_fvc, predicted_fvc, confidence, return_values = False):\n",
    "    \"\"\"\n",
    "    Calculates the modified Laplace Log Likelihood score for this competition.\n",
    "    \"\"\"\n",
    "    sd_clipped = np.maximum(confidence, 70)\n",
    "    delta = np.minimum(np.abs(actual_fvc - predicted_fvc), 1000)\n",
    "    metric = - np.sqrt(2) * delta / sd_clipped - np.log(np.sqrt(2) * sd_clipped)\n",
    "\n",
    "    if return_values:\n",
    "        return metric\n",
    "    else:\n",
    "        return np.mean(metric)\n",
    "\n",
    "def make_model(nh):\n",
    "    z = L.Input((nh,), name=\"Patient\")\n",
    "    x = L.Dense(100, activation=\"relu\", name=\"d1\")(z)\n",
    "    x = L.Dense(100, activation=\"relu\", name=\"d2\")(x)\n",
    "    p1 = L.Dense(3, activation=\"linear\", name=\"p1\")(x)\n",
    "    p2 = L.Dense(3, activation=\"relu\", name=\"p2\")(x)\n",
    "    preds = L.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), \n",
    "                     name=\"preds\")([p1, p2])\n",
    "    \n",
    "    model = M.Model(z, preds, name=\"CNN\")\n",
    "    #model.compile(loss=qloss, optimizer=\"adam\", metrics=[score])\n",
    "    model.compile(loss=mloss(0.8), optimizer=tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False), metrics=[score])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET TRAINING DATA AND TARGET VALUE\n",
    "\n",
    "# get target value\n",
    "y  = tr['FVC'].values\n",
    "\n",
    "# get training & test data\n",
    "X_train = tr[FE].values\n",
    "X_test = sub[FE].values\n",
    "\n",
    "\n",
    "nh = X_train.shape[1]\n",
    "\n",
    "# instantiate target arrays\n",
    "train_preds = np.zeros((X_train.shape[0], 3))\n",
    "test_preds = np.zeros((X_test.shape[0], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Patient (InputLayer)           [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " d1 (Dense)                     (None, 100)          1000        ['Patient[0][0]']                \n",
      "                                                                                                  \n",
      " d2 (Dense)                     (None, 100)          10100       ['d1[0][0]']                     \n",
      "                                                                                                  \n",
      " p1 (Dense)                     (None, 3)            303         ['d2[0][0]']                     \n",
      "                                                                                                  \n",
      " p2 (Dense)                     (None, 3)            303         ['d2[0][0]']                     \n",
      "                                                                                                  \n",
      " preds (Lambda)                 (None, 3)            0           ['p1[0][0]',                     \n",
      "                                                                  'p2[0][0]']                     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,706\n",
      "Trainable params: 11,706\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "11706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "net = make_model(nh)\n",
    "print(net.summary())\n",
    "print(net.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2315., 2214., 2061., ..., 2908., 2975., 2774.], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000014F8A638A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000014F8A638A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function mloss.<locals>.loss at 0x0000014F89B04678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function mloss.<locals>.loss at 0x0000014F89B04678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function score at 0x0000014F89C790D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function score at 0x0000014F89C790D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000014F8A664DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000014F8A664DC8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "train [39.24629211425781, 6.463183403015137]\n",
      "val [40.77925109863281, 6.5740966796875]\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000014F8C5C9EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000014F8C5C9EE8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "predict test...\n",
      "FOLD 2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000014F8C676438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000014F8C676438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function mloss.<locals>.loss at 0x0000014F8C676D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function mloss.<locals>.loss at 0x0000014F8C676D38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000014F8EFC8438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000014F8EFC8438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "train [37.79692840576172, 6.424067497253418]\n",
      "val [55.92242431640625, 6.849629878997803]\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000014F922C60D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000014F922C60D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "predict test...\n",
      "FOLD 3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000014F922C6438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000014F922C6438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function mloss.<locals>.loss at 0x0000014F92654CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function mloss.<locals>.loss at 0x0000014F92654CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000014F93ADD8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000014F93ADD8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "train [39.55009841918945, 6.501192569732666]\n",
      "val [54.81645202636719, 6.98070764541626]\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000014F94748E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000014F94748E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "predict test...\n",
      "FOLD 4\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000014F9485A798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000014F9485A798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function mloss.<locals>.loss at 0x0000014F9485A168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function mloss.<locals>.loss at 0x0000014F9485A168> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000014F96AD19D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000014F96AD19D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "train [40.807464599609375, 6.531225681304932]\n",
      "val [47.61213684082031, 6.654099941253662]\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000014F97254AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000014F97254AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "predict test...\n",
      "FOLD 5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000014F97259E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000014F97259E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function mloss.<locals>.loss at 0x0000014F97259A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function mloss.<locals>.loss at 0x0000014F97259A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000014F97D2A5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000014F97D2A5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [40.77476501464844, 6.516221523284912]\n",
      "val [46.2763557434082, 6.68809700012207]\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000014F97F6B678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000014F97F6B678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "predict test...\n"
     ]
    }
   ],
   "source": [
    "model_cnt = 1\n",
    "# instantiate target arrays\n",
    "globals()['train_preds_{}'.format(model_cnt)] = np.zeros((X_train.shape[0], 3))\n",
    "globals()['test_preds_{}'.format(model_cnt)] = np.zeros((X_test.shape[0], 3))\n",
    "\n",
    "NFOLD = 5\n",
    "gkf = GroupKFold(n_splits=NFOLD)\n",
    "groups = tr['Patient'].values\n",
    "\n",
    "cnt = 0\n",
    "EPOCHS = 800\n",
    "BATCH_SIZE=128\n",
    "\n",
    "for tr_idx, val_idx in gkf.split(X_train,y, groups):\n",
    "    cnt += 1\n",
    "    print(f\"FOLD {cnt}\")\n",
    "    net = make_model(nh)\n",
    "    net.fit(X_train[tr_idx], y[tr_idx], batch_size=BATCH_SIZE, epochs=EPOCHS,validation_data=(X_train[val_idx], y[val_idx]), verbose=0) #\n",
    "    print(\"train\", net.evaluate(X_train[tr_idx], y[tr_idx], verbose=0, batch_size=BATCH_SIZE))\n",
    "    print(\"val\", net.evaluate(X_train[val_idx], y[val_idx], verbose=0, batch_size=BATCH_SIZE))\n",
    "        #print(\"predict val...\")\n",
    "    globals()['train_preds_{}'.format(model_cnt)][val_idx] = net.predict(X_train[val_idx], batch_size=BATCH_SIZE, verbose=0)\n",
    "    print(\"predict test...\")\n",
    "    globals()['test_preds_{}'.format(model_cnt)] += net.predict(X_test, batch_size=BATCH_SIZE, verbose=0) / NFOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Score:  -6.711942603937465\n"
     ]
    }
   ],
   "source": [
    "predicted_fvc = globals()['train_preds_{}'.format(model_cnt)][:,1]\n",
    "confidence = globals()['train_preds_{}'.format(model_cnt)][:,2]-globals()['train_preds_{}'.format(model_cnt)][:,0]\n",
    "model_score = laplace_log_likelihood(actual_fvc = y, predicted_fvc = predicted_fvc, confidence = confidence,\n",
    "                       return_values = False)\n",
    "print('Overall Score: ', model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4820\\2642347020.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'submission_kaggle.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_csv('submission_kaggle.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4820\\2785872833.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msigma_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0munc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msigma_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma_mean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "sigma_opt = mean_absolute_error(y, pred[:, 1])\n",
    "unc = pred[:,2] - pred[:, 0]\n",
    "sigma_mean = np.mean(unc)\n",
    "print(sigma_opt, sigma_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.random.randint(0, y.shape[0], 100)\n",
    "plt.plot(y[idxs], label=\"ground truth\")\n",
    "plt.plot(pred[idxs, 0], label=\"q25\")\n",
    "plt.plot(pred[idxs, 1], label=\"q50\")\n",
    "plt.plot(pred[idxs, 2], label=\"q75\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unc.min(), unc.mean(), unc.max(), (unc>=0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(unc)\n",
    "plt.title(\"uncertainty in prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION\n",
    "sub['FVC1'] = 1.*pe[:, 1]\n",
    "sub['Confidence1'] = pe[:, 2] - pe[:, 0]\n",
    "subm = sub[['Patient_Week','FVC','Confidence','FVC1','Confidence1']].copy()\n",
    "subm.loc[~subm.FVC1.isnull()].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.loc[~subm.FVC1.isnull(),'FVC'] = subm.loc[~subm.FVC1.isnull(),'FVC1']\n",
    "sigma_mean = 60\n",
    "if sigma_mean<sigma_mean:\n",
    "    subm['Confidence'] = sigma_opt\n",
    "else:\n",
    "    subm.loc[~subm.FVC1.isnull(),'Confidence'] = subm.loc[~subm.FVC1.isnull(),'Confidence1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otest = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/test.csv')\n",
    "for i in range(len(otest)):\n",
    "    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'FVC'] = otest.FVC[i]\n",
    "    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'Confidence'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission_regression.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_sub = subm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = img_sub.sort_values(by=['Patient_Week'], ascending=True).reset_index(drop=True)\n",
    "df2 = reg_sub.sort_values(by=['Patient_Week'], ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1[['Patient_Week']].copy()\n",
    "df['FVC'] = (0.45*df1['FVC'] + 0.55*df2['FVC'])\n",
    "df['Confidence'] = (0.45*df1['Confidence'] + 0.55*df2['Confidence'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('submission_kaggle.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
